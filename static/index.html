<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <title>Gesture→MIDI Demo</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.5/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-SgOJa3DmI69IUzQ2PVdRZhwQ+dy64/BUtbMJw1MZ8t5HZApcHrRKUc4W0kG879m7" crossorigin="anonymous">
  <style>
    body {
      margin: 20px;
    }

    #video,
    #vcanvas {
      border: 1px solid #ccc;
      margin: 8px;
    }

    #vcanvas {
      display: none;
    }
  </style>
</head>

<body>
  <div class="container">
    <h1>Gesture→MIDI Live Demo</h1>
    <button type="button" class="btn btn-primary mb-4" data-bs-toggle="modal" data-bs-target="#settingsModal">
      Settings
    </button>
    <video id="video" class="w-100 mb-4" autoplay playsinline></video>
  </div>

  <div class="modal fade" id="settingsModal" tabindex="-1" role="dialog" aria-labelledby="settingsModalLabel"
    aria-hidden="true">
    <div class="modal-dialog" role="document">
      <div class="modal-content">
        <div class="modal-header">
          <h5 class="modal-title" id="settingsModalLabel">Settings</h5>
          <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close">
            <span aria-hidden="true"></span>
          </button>
        </div>
        <div class="modal-body">
          <!-- MIDI Output selection -->
          <div class="form-group">
            <label for="midiSelect">MIDI Output:</label>
            <select id="midiSelect" class="form-control">
              <option value="builtin">Built-in Synthesizer</option>
            </select>
          </div>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
          <button type="button" class="btn btn-primary" id="saveSettings">Save Settings</button>
        </div>
      </div>
    </div>
  </div>
  <canvas id="vcanvas"></canvas>
  <script type="module">
    import {createGestureMidiRunner} from '/inference.js';

    // Stub: replace with real MIDI‐output logic
    function consumeMIDI(note, velocity) {
      console.log(`MIDI → note: ${note}, velocity: ${velocity}`);
      // e.g. send to Web MIDI API or WebAudio synth
    }

    (async () => {
      // 1) Set up media streams
      const stream = await navigator.mediaDevices.getUserMedia({video: true, audio: true});
      const videoEl = document.getElementById('video');
      videoEl.srcObject = stream;

      // 2) Create audio context & ScriptProcessor for PCM frames
      const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      const mic = audioCtx.createMediaStreamSource(stream);
      //  bufferSize must be a power of 2: 1024 samples ≈ 23ms @ 44.1kHz
      const processor = audioCtx.createScriptProcessor(1024, 1, 1);
      mic.connect(processor);
      processor.connect(audioCtx.destination);

      // 3) Prepare offscreen canvas for video frame resizing
      const vcanvas = document.getElementById('vcanvas');
      const vctx = vcanvas.getContext('2d');

      // 4) Load the TF.js runner
      const runner = await createGestureMidiRunner('/models/gesture_to_midi/model.json');

      let latestAudioBuffer = null;
      // On each audio process, capture PCM float32 data
      processor.onaudioprocess = ev => {
        const input = ev.inputBuffer.getChannelData(0);
        // Copy into a Float32Array for TensorFlow.js
        latestAudioBuffer = new Float32Array(input);
      };

      // 5) Main loop: grab latest audio & video, run inference + plasticity
      async function frameLoop() {
        if (latestAudioBuffer) {
          // --- Prepare audio tensor: [timeSteps, 1]
          const audioTensor = tf.tensor(latestAudioBuffer, [latestAudioBuffer.length, 1]);

          // --- Prepare video tensor: capture & resize frame to 224×224
          vctx.drawImage(videoEl, 0, 0, vcanvas.width, vcanvas.height);
          const imgData = vctx.getImageData(0, 0, vcanvas.width, vcanvas.height);
          // tf.browser.fromPixels returns [H,W,3], we need [D,H,W,3] with D=1
          const videoTensor = tf.tidy(() => {
            const px = tf.browser.fromPixels(imgData).toFloat().div(255);
            return px.expandDims(0);  // shape [1,224,224,3]
          });

          // --- Run inference + Hebbian update
          const {noteLogits, veloLogits} = await runner.step(audioTensor, videoTensor);

          // --- Decode logits to argmax
          const note = noteLogits.argMax().arraySync();
          const velocity = veloLogits.argMax().arraySync();
          consumeMIDI(note, velocity);

          // Clean up
          audioTensor.dispose();
          videoTensor.dispose();
          noteLogits.dispose();
          veloLogits.dispose();
        }
        requestAnimationFrame(frameLoop);
      }

      // Start!
      frameLoop();
    })();
  </script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.5/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-k6d4wzSIapyDyv1kpU366/PK5hCdSbCRGRCMv+eplOQJWyd1fbcAu9OCUj5zNLiq"
    crossorigin="anonymous"></script>
</body>

</html>
